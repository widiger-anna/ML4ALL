[
{"p_id":"1", "p_text":"En una serie de publicaciones anteriores, hemos analizado algunas ideas generales relacionadas con las tareas de la ciencia de datos textuales, ya sea el procesamiento del lenguaje natural, la minería de textos o algo diferente pero estrechamente relacionado. En la más reciente de estas publicaciones , cubrimos un tutorial de preprocesamiento de datos de texto usando Python. Más específicamente, miramos un tutorial de preprocesamiento de texto usando Python y NLTK . Si bien no fuimos más allá del preprocesamiento de datos con NLTK, el kit de herramientas podría, teóricamente, ser utilizado para otras tareas de análisis."},
{"p_id":"2", "p_text":"Si bien NLTK es un gran lenguaje natural … bueno, el juego de herramientas (de ahí el nombre), no está optimizado para la construcción de sistemas de producción. Esto puede o no tener consecuencias si usa NLTK solo para preprocesar sus datos, pero si está planificando una solución PLN de extremo a extremo y está seleccionando una herramienta apropiada para construir dicho sistema, puede tener sentido preprocesar sus datos con lo mismo."},
{"p_id":"3", "p_text":"Si bien NLTK se creó teniendo en cuenta el aprendizaje de PLN, spaCy se diseñó específicamente con el objetivo de ser una biblioteca útil para implementar sistemas listos para producción."},
{"p_id":"4", "p_text":"spaCy está diseñado para ayudarlo a realizar un trabajo real: para crear productos reales o recopilar información real. La biblioteca respeta su tiempo y trata de evitar desperdiciarlo. Es fácil de instalar y su API es simple y productiva. Nos gusta pensar en spaCy como el Ruby on Rails del procesamiento del lenguaje natural."},
{"p_id":"5", "p_text":"spaCy es obstinado, ya que no permite mezclar y combinar lo que podría considerarse módulos de tuberías (pipelines) PLN, y el argumento es que los lemmatizadores particulares, por ejemplo, no están optimizados para funcionar bien con tokenizadores particulares. Si bien la compensación es menor flexibilidad en algunos aspectos de su canalización PLN, el resultado debería ser un mayor rendimiento."},
{"p_id":"6", "p_text":"Puede obtener una descripción general de SpaCy y algunas de sus filosofías de diseño y opciones en este video , una charla impartida por los desarrolladores de spa Matthew Matthew y Ines Montani , de una reunión de SF Machine Learning coorganizada por el Data Institute de USF."},
{"p_id":"7", "p_text":"spaCy se anuncia a sí mismo como “la mejor manera de preparar el texto para el aprendizaje profundo”. Como gran parte del tutorial anterior no usó NLTK (la eliminación del ruido dependiente de la tarea, así como algunos pasos en el proceso de normalización), no repetiremos todo el post aquí usando spaCy en lugar de NLTK en lugares específicos, ya que sería una pérdida de tiempo para todos. En su lugar, investigaremos cómo se podría lograr con SpaCy parte de la misma funcionalidad con la que trabajamos NLTK, y luego pasaremos a algunas tareas adicionales que podrían considerarse preprocesamiento, dependiendo de su objetivo final, pero que también pueden sangrar en los pasos subsiguientes de tareas de ciencia de datos textuales."},
{"p_id":"8", "p_text":"Antes de hacer nada, debe tener SpaCy instalado, así como su modelo de idioma inglés."},
{"p_id":"9", "p_text":"Necesitamos una muestra de texto para usar:"},
{"p_id":"10", "p_text":"Ahora importemos spaCy, junto con displaCy (para visualizar algunos de los modelos de spaCy) y una lista de palabras de finalización en inglés (las usaremos a continuación). También cargamos el modelo de idioma inglés como un Languageobjeto (lo llamaremos 'nlp' fuera de la convención spaCy), y luego llamamos al objeto nlp en nuestro texto de muestra, que devuelve un Docobjeto procesado (que inteligentemente llamamos 'doc')."},
{"p_id":"11", "p_text":"Y ahí lo tienes. De la documentación de spaCy :"},
{"p_id":"12", "p_text":"A pesar de que a Docse procesa, por ejemplo, se divide en palabras individuales y se anota, todavía contiene toda la información del texto original , como los caracteres en blanco. Siempre puede obtener el desplazamiento de un token en la cadena original, o reconstruir el original uniendo los tokens y sus espacios en blanco al final. De esta forma, nunca perderá ninguna información al procesar texto con spaCy."},
{"p_id":"13", "p_text":"Esto es esencial para la filosofía de diseño de spaCy; Te animo a mirar este video ."},
{"p_id":"14", "p_text":"Ahora probemos algunas cosas. Tenga en cuenta qué tan poco código se requiere para realizar dichas cosas."},
{"p_id":"15", "p_text":"Imprimir tokens de nuestra muestra es sencillo:"},
{"p_id":"16", "p_text":"Recuerde desde arriba que un Docobjeto se procesa a medida que se pasa al Languageobjeto, y así la tokenización ya se ha completado en este punto; solo estamos accediendo a los tokens existentes. De la documentación :"},
{"p_id":"17", "p_text":"Un Doc es una secuencia de Tokenobjetos. Acceda a oraciones y entidades con nombre, exporte anotaciones a matrices numpy, serialice sin pérdidas a cadenas binarias comprimidas. El Docobjeto contiene una matriz de TokenCestructuras. El nivel de Python Tokeny los Spanobjetos son vistas de esta matriz, es decir, no son propietarios de los datos."},
{"p_id":"18", "p_text":"No discutimos los detalles de la implementación de spaCy, pero al conocer cómo es su código subyacente, específicamente cómo se escribió (y por qué), se obtiene la información necesaria para comprender por qué es tan rápido. Por tercera vez, te animo a mirar este video ."},
{"p_id":"19", "p_text":"Aunque no es necesario, dadas las diferencias de diseño, si desea extraer estos tokens en una lista propia (similar a como lo hicimos en el tutorial NLTK):"},
{"p_id":"20", "p_text":"Identificando las StopWords (palabras muy comunes en el lenguaje sin significado importante)Identifiquemos las StopWords. Importamos la lista de palabras anterior, por lo que solo se trata de iterar a través de los tokens almacenados en el Docobjeto y realizar una comparación:"},
{"p_id":"21", "p_text":"Tenga en cuenta que no tocamos la lista de tokens creada anteriormente, y solo confiamos en el Docobjeto preprocesado ."},
{"p_id":"22", "p_text":"Un Docobjeto contiene Tokenobjetos, y puede leer la Token documentación para obtener una idea de qué datos posee cada ficha antes de que la solicitemos, y para obtener información específica sobre lo que se muestra a continuación."},
{"p_id":"23", "p_text":"Si bien esta es una gran cantidad de información tal como está, y podría ser útil en todo tipo de formas para un objetivo PLN determinado, visualicemos simplemente usando displaCy para obtener una vista más concisa:"},
{"p_id":"24", "p_text":"Un Docobjeto ya ha procesado entidades con nombre también. Acceda a ellos con:"},
{"p_id":"25", "p_text":"Como puede determinarse a partir del código anterior, primero se genera la entidad nombrada, seguido de su índice de caracteres inicial en el documento, luego su índice de carácter final y finalmente su tipo de entidad (etiqueta)."},
{"p_id":"26", "p_text":"La visualización de displaCy es útil de nuevo, aquí:"},
{"p_id":"27", "p_text":"Reitero: recorra esta publicación y observe qué tan poco código se requería para llevar a cabo las tareas que llevamos a cabo."},
{"p_id":"28", "p_text":"Esto solo araña la superficie de lo que queremos lograr con PLN o una herramienta tan poderosa como SpaCy. Como debería ser obvio, las tareas de preprocesamiento de PLN, así como las de otras formas de datos (junto con la manipulación de texto y cadena), a menudo se pueden lograr con una variedad de estrategias, herramientas y bibliotecas. En mi opinión, spaCy sobresale debido a su facilidad de uso y simplicidad API. Animo a todas las personas que han estado siguiendo estos mensajes relacionados con la PLN que he estado escribiendo a que consulten SpaCy si se toman en serio el procesamiento del lenguaje natural."},
{"p_id":"29", "p_text":"Para obtener más información sobre spaCy, recomiendo estos recursos:"}
]
